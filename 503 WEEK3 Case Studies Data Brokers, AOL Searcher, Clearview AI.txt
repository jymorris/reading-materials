Hello again, in this segment we're going to move from the beginning of the lifecycle of data to the end. Those are both key points where we're going to interrogate this question about, is data science inherently or likely systematically to leave certain things or people out? We're also going to focus more on the second framing question for this week. Which is about whether or not data can ever really be said to be portable? So the issue that we raised in the previous segments for this week was really about where data come from and how you get data? What we'd like to do now is link that idea to where data end up. So we were talking about the beginning, now we're talking about the end. The way we might think about that is using the phrase aggregation. So aggregation just means putting data together with other data, obviously, maybe obviously. The reason I want to highlight it for you is that aggregation is a little bit like a landmine, or a pitfall, or whatever metaphor you want in data science ethics. Because whenever you merge data together, it's kind of very likely that it will become a little harder to disentangle the provenance of particular rows or instances of data in the dataset. So let's talk through some examples. And so the reason I have called this particular segment provenance and aggregation is that, I want to point out how way at the end of the data lifecycle, you can sometimes make moves that make it hard to see what problems happened at the beginning and that's a good example. So as an example, I have here a screenshot of axicom, acxiom, excuse me. Acxiom, I think it's Acxiom, yeah, which is a data broker. I've used data brokers in my research in the past, although not this one. I just want to highlight that there's an ecology of data brokers that if you haven't been working in this area you might not be aware of. But there are companies whose business model is aggregation. So their job is to find data about people and to add this all together into one massive bucket of data that they then resell. And so they have a variety of inputs including stuff from social media sites, stuff from studies that they commission, stuff from public records, stuff from, one of the key sources of data in the United States is credit cards. Credit cards are a huge source of data for the data broker industry. And in this front page of the Acxiom website, they emphasize marketing but people do use these data for all kinds of purposes. Interesting thing about Acxiom and these companies, I didn't use Acxiom but I used a competitor to them. And I was kind of baffled as an analyst when I started using their interface. Because one of the things that they do is they offer ways to categorize people and it's a very, very long list. But generally maybe because I don't want to say terrible things about marketers, but I might have to. So I think marketers tend to not have as much data science experience and I think that was the target for the interface I was using. So marketing tends not to be as quantitatively grounded as some other disciplines. So immediately for me as an analyst, the way that the data were aggregated raised all kinds of flags and concerns. So some of them raised concerns because I was kind of concerned that there were variables in there at all. So I was surprised that there were rows for things like sexual orientation which could be seen as against the law in some United States depending on how you use that. And there was no warning that popped up or anything about. It didn't say be careful, race, be careful, don't use this in this way, or it's a crime. It just was a giant list of data and you could pull out what you needed. So I was surprised that some data existed. But then there were so many, there were hundreds, and hundreds, and hundreds of variables you can buy from Acxiom and the like. And many of these variables really have no explanation. So you can click and click and there's really not any kind of explanation of provenance in the data. So some of them, as a data scientist [LAUGH] and a professor, I have to say I'm pretty skeptical. So some of the rows in the dataset that I examined were things like likeliness to buy a particular product in the next seven days and things like that. Or interested in moving away to a different state or something like that. And so these are pretty notoriously hard to measure in some cases and it would really help if there was some indication of how you were getting this. Is it that people were searching for moving companies or is it that they filed a change of address form which is a public record that they might use? These are different kinds of data, or was it added together in some sort of predicted model? There really was no accountability. So I just want to highlight that one of the problems is that because there is no connection from the source to the use of the data, it just makes it very difficult for the analysis to know if you're going to do a good job with this. If you're going to come up with the right answer or who you're leaving out. So I guess this is just a warning about aggregation as a point that tends to obscure accountability and also provenance origination and makes these kinds of ethical problems harder to solve. In some jurisdictions, aggregation itself may be illegal in the European Union for example. Data protection laws protect people from their data being resold. In the United States, that generally is not the case, and so we have a little bit of a different experience here. But as an analyst who feels a responsibility to come up with good analysis and the right answers, a lot of the aggregated data I think is just sort of garbage. So watch out for that. They'll charge you for it too. So let's go through a couple of interesting case studies here, I have two. This is a somewhat famous case, even though it's an older case, it's sometimes called AOL Searcher 4417749, pretty [LAUGH] catchy. This is a famous case from a long time ago, because at one time researchers thought that it was ok to anonymize data just by taking the names off. There really was not as much knowledge about data anonymization and the risks that were posed by poor anonymization. And really now we think of anonymization as very difficult and perhaps impossible, but at the time this was not the case. So in 2006, some researchers at an internet service provider called America Online decided to release the web search histories of their users to further academic research about search engines. And so they removed the identities of the searchers and assigned all of them a number, 4417749 was one of the searchers. And the reason this became an interesting data science ethics case, is that reporters at the New York Times were reading through this dataset. And they thought it's really quite easy to identify people from their search history even if their names are removed. And I don't know if you're aware, but your search history is extremely personal. So here is some of the search history from Searcher number 4417749. You see dog urinates on everything, foods to avoid when breastfeeding, bipolar, depression and medical leave, fear that spouse contemplating cheating. These are things that this person was searching for. The New York Times actually knocked on the door of this person, was able to determine their address and find them. And I think maybe to protect themselves, they agreed to participate in the story which you can look up. But they said they did a lot of searching for their friends. I don't know if that's true or not, but it's a convenient canard that would give you some sort of after the fact deniability for your search history. You might have a look at your search history. Google for example allows you to examine your own search history if you dig into the settings. There's a page for it and it's really quite revealing about you. I think it's an important case because it shows that, it's hard for me to believe. But it shows that at one time people were just not able to see that this was identifying information, this was sensitive. And so the earlier way of thinking about this was that you would retain data forever or as long as you wanted to and it was totally up to you. And that people really didn't have rights to their data, so if you wanted to resell it, that you could. And I think we're now in a moment where that is changing. And sort of the beginning case that's somewhat well known in data science of that change was this 2006 Searcher, AOL Searcher 4417749. One other thing I'd highlight about this case is that here in the United States for example, earlier technologies have had extremely different sort of laws and mental models for how we think about the disposition of data. So for example in the United States, if you go to a public library and check out a book, you might check out a book that tells a lot about you. So a famous book in the United States is called the Anarchists Cookbook. And it's a manual for bomb making, and it's a quite popular book. It's available in your public library, I would bet. Because legislators were concerned about law enforcement's ability to know things about you via the books you check out, that book perhaps is particularly worrisome to law enforcement. They passed a particular law that says that if you're a librarian you cannot disclose the borrowing record of someone at a public library without consent. And so it's interesting that this was such a concern then. But as we move into the internet, this law seems quite out of step with what's currently happening to our data. Remember that Google can store and sell, and release all kinds of data like your search history. Which I think I would argue is somewhat equivalent to what the legislature was concerned about with this restriction on library records. So currently if I checked the book, the Anarchist Cookbook out of the library, I would have this legal protection for no one knowing about it. But if I typed in the Anarchist Cookbook, I would not have that protection if I typed into Google. Let's do one more example. So this is again sort of rather than looking at where the data originated, you look at where it ends up. This is a very recent New York Times study, story, [LAUGH] excuse me, about a company that, the headline says, The Secretive Company That Might End Privacy as We Know It. So it's a very successful company in terms of hype, called Clearview. The motto is, technology to help solve the hardest crimes, here's their web page. And it's sometimes called Clearview AI. It's interesting because it's a facial recognition technology company that offers its services to law enforcement. And the interesting thing about it is that it gathers all of its data about people that is then analyzed and resold to law enforcement from public information on social media. So for example, here's a picture of a face detection algorithm running on creative commons image from Wikipedia. That's the Wikipedia founder, Jimmy Wales. So if you typed in, if you showed, excuse me, not typed in. If you showed Clearview AI a picture of Wikipedia founder, Jimmy Wales, it would try to identify him on the basis of this photograph which was available publicly. One of the shocking things about Clearview AI is that it violated all of the terms of service of social media companies in order to assemble this dataset. So Clearview AI would scrape public data from anything that was available regardless of what legal terms were attached to that data. So for example if you made a settings change on your social media account like your Instagram profile or you made a settings change on your Facebook profile and it became public, it's quite possible that Clearview AI scrapers would grab all of that data and use any faces that they found as grist for their analysis. That's tricky because the companies involved have user agreements and the user agreements specify that you're not allowed to do any scraping for example. And you're not allowed to gather information for purposes specifically like this one from social media, but Clearview AI did it anyway. So in the New York Times story, they ran the journalist's face and found a bunch of images from social media. And he was quite surprised by how effective it is that in fact the company has promoted these testimonials from law enforcement that says, we had these unsolved crimes, ran them through Clearview AI, came out with the right answer. So it's interesting, the question I have for you is what is the ethical dimension to the data here? Because the data was created for one purpose, in this case, illustrating a panel discussion about Wikipedia. It was then scraped and it was scraped in a way that in many instances was intentionally against the rules, but it's not super clear how to enforce those rules in the United States context. So you violate the terms of service and there's some legal ambiguity as to the consequences of that. So Clearview AI seems to be going on kind of a business plan that is, we got the data, so we've got it, and we're going to use it. And so maybe it came from some other thing for some other purpose, but we're going to go ahead and use it. The challenge made by the New York Times reporting is that we have never as a society really thought about information in this way. Or the idea that if something was ever made public online, that it now goes into one giant database. I don't know how good their scrapers are, so I might be a little bit of a hyperbole. But we we had a certain meaning assumed with the word public, like when I make something public. But Clearview AI, their meaning of public is, was it ever public? And no matter under what conditions, it all just goes into the hopper. So the issue there is that after the data has been repurposed without permission of the users or the people pictured, do you have any recourse? State laws on this vary, there are actually some state laws in the United States. I understand in Illinois particularly, that might have some bearing on this. But really I think the challenge is that it's beyond the pale in terms of its ethical implications. We haven't got a good rationale for how to put this genie back inside the box now that there is this alleged dataset that has all this stuff in it. And so people are quite worried. I'll stop there.